#' Title
#'
#' @param data A data.frame(or coercible to data.frame)
#' @param formula The formula for the regression
#' @param tau The quantile
#' @param n_bootstrap The number of resamples for bootstrap correction. Default is NULL,
#' so no boostrap correction occurs.
#'
#' @return The linear model to predict the values of mu
#'
#' @export
#' @importFrom stats optim model.matrix lm.fit
#' @import checkmate
#' @examples
#'
reg_chen <- function(data, formula, tau = 0.5, n_bootstrap = NULL){ # For the reparametrized distribution only
  tryCatch(data <- as.data.frame(data),
           error = function(e) stop("The object provided as data is not coercible to data.frame"))
  checkmate::assert_data_frame(data, any.missing = F)
  checkmate::assert_formula(formula)
  checkmate::assert_number(tau, lower = 0, upper = 1)
  checkmate::assert_number(n_bootstrap, lower = 1, null.ok = T)

  #______________________________________DATA_CLEANING____________________________________

  data <- as.data.frame(data)

  y <- as.vector(data[, all.vars(formula[[2]])])
  X <- stats::model.matrix(formula, data)
  beta_start <- stats::lm.fit(as.matrix(X), unlist(log(y)))$coefficients
  # The gradient is suplied for the sake of optimization only,
  # if it was not supplied stats::optim would estimate it numerically
  # , so we can avoid the extra computation by calculating it manually

  # 0.7 starting point for lambda is arbitrary, just as the parameters for the regression
  initial_par <- c(0.7, beta_start)
  #_______________________________________BOOTSTRAP_______________________________________

  if(!is.null(n_bootstrap)){
    error_count <- 0
    boot_func_reg <- function(y, indices, X, initial_par) { # Function that receives the indices as generated by boot::boot to take a sample from the data
      bootstrap_y <- y[indices]
      bootstrap_X <- X[indices, ]

      tryCatch({suppressWarnings(bootstrap_estim <- stats::optim(par = initial_par,
                                                                   fn = log_likelihood_rpr_reg,
                                                                   y = bootstrap_y,
                                                                   X = bootstrap_X,
                                                                   tau = tau,
                                                                   method = "BFGS",
                                                                   hessian = F,
                                                                   control = list(fnscale = -1)))
        return(bootstrap_estim$par)
      }, error = function(e) {
        error_count <<- error_count + 1
        return(rep(-1, length(initial_par)))
      })
    }
  boot_results <- boot::boot(data = y, statistic = boot_func_reg, R = n_bootstrap, X = X, initial_par = initial_par) # Where the bootstrap actually occurs
  boot_results$t <- boot_results$t[!(boot_results$t[, 1] == -1), ]
  boot_results$R <- boot_results$R - error_count
  # First part of the return
  pars <- colMeans(boot_results$t)
  model <- list()
  model$names <- colnames(X)
  model$coef0 <- boot_results$t0[2 : length(boot_results$t0)]
  model$coef <- pars[2:length(pars)]
  model$lambda <- pars[1]
  model$tau <- tau
  model$non_convergent_samples <- error_count
  model$formula <- formula
  model$y <- y
  model$X <- X
  model$call <-  match.call()
  class(model) <- "reg_chen_model"
  return(model)
#______________________________________NO_BOOTSTRAP_______________________________________
  } else {
    estim <- stats::optim(par = initial_par,
                          fn = log_likelihood_rpr_reg,
                          hessian = T,
                          control = list(fnscale = -1, reltol = 1e-10), # Maybe test maxit arguments
                          gr = escore,
                          y = y,
                          X = X,
                          tau = tau)

    if(estim$convergence != 0){
      stop("The optimization did not converge!!!!
      The convergence value was: ", estim$convergence, ". Try looking at stats::optim documentation to see what this value means")
    }
  }
  #First part of the return
  model <- list()
  model$names <- colnames(X)
  model$coef <- estim$par[2: length(estim$par)]
  model$lambda <- estim$par[1]
  model$tau <- tau
  model$formula <- formula
  model$y <- y
  model$X <- X
  model$call <-  match.call()
  class(model) <- "reg_chen_model"
  return(model)
}

log_likelihood_rpr_reg <- function(y, theta, X, tau){
  lambda <- theta[1]

  betas <- theta[2:length(theta)]
  mu_hats <- exp(X %*% as.matrix(betas))

  ll <- suppressWarnings(log(log(1 - tau) / (1 - exp(mu_hats^lambda))) +
                           log(lambda) + (lambda - 1) * log(y) +
                           (log(1 - tau) / (1 - exp(mu_hats^lambda))) * (1 - exp(y^lambda)) + (y^lambda))
  return(sum(ll))
}

escore <- function(y, theta, X, tau) {
  lambda <- theta[1]
  beta <- theta[2:length(theta)]

  linear_predictor <- as.vector(X %*% as.matrix(beta))
  exponentiated_predictor <- exp(linear_predictor)

  beta_contribution <- as.vector(-(lambda * exponentiated_predictor^(lambda - 1) *
                                     exp(exponentiated_predictor^lambda) * (exp(exponentiated_predictor^lambda) +
                                                                              log(1 - tau) * exp(y^lambda) - log(1 - tau) - 1)) / ((exp(exponentiated_predictor^lambda) - 1)^2))

  lambda_contribution <- as.vector(((-log(1 - tau) * y^lambda * log(y) * exp(y^lambda) +
                                       (exponentiated_predictor^lambda) * log(exponentiated_predictor) * exp(exponentiated_predictor^lambda)) /
                                      (1 - exp(exponentiated_predictor^lambda))) + ((log(1 - tau) * (exponentiated_predictor^lambda) *
                                                                                       log(exponentiated_predictor) * exp(exponentiated_predictor^lambda) * (1 - exp(y^lambda)))
                                                                                    / ((1 - exp(exponentiated_predictor^lambda))^2)) + 1 / lambda + y^lambda * log(y) + log(y))

  tau_matrix <- diag(exp(linear_predictor))
  lambda_sum <- sum(lambda_contribution)
  beta_product <- t(X) %*% tau_matrix %*% beta_contribution

  result_vector <- c(lambda_sum, beta_product)
  return(result_vector)
}
